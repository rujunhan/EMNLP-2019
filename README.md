**Author**: Rujun Han

**Date**: Nov 22nd, 2019

**Title**: Codebase for EMNLP 2019 Paper: [Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction](https://www.aclweb.org/anthology/D19-1041.pdf) 

1. Data processinng. We have preprocessed TB-Dense and MATRES raw data using internal NLP tools at the Information Sciences Institute. These .pickle files are save in data fold.
2. Featurize data. Run featurize_data.py and context_aggregator.py sequentially. Two folders are created: all_joint/ and all_context/. all_context contains the final files used in the model.
3. Local Model: run joint_model.py
4. Global Model: save a pipeline_joint model object from step and then run joint_model.py.


### Code Structure (joint_model.py)

Main() --> [NNClassifier].train_epoch()

[NNClassifier].train_epoch() --> [NNClassifier]._train()

-------------------------------> [NNClassifier].predict()


1. Singletask Model. Set args.relation_weights = 0 to get event module; set args.entity_weights = 0; to train relation model; use both saved modules to train a pipeline end-to-end model.
2. Multitask Model. Set args.pipe_epoch = 1000,  args.eval_gold = True to train with gold relations on
ly; set args.eval_gold = False to train with candidate relations generated by event module.
3. Pipeline Joint Model. Set args.pipe_epoch < args.epochs and set args.eval_gold = False to train with candidate relations generated by event module. Our paper used the output model in this step as local model.
4. Global model. Install Gurobi package and run joint_model_global.py
